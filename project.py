# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hbp9Xd-5gJylPs4c_lyrqrOh7dPOX9r-
"""

import torch
import numpy as np
import torch.nn as nn

!pip install transformers

from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

txt = '27 November 1926Dont you know that there is nothing I wouldnt do to save you a moments pain annoyance fatigue irritationVita'
marked_txt = "[CLS]" + txt.lower() + "[SEP]"

token_text = tokenizer.tokenize(marked_txt)

print(token_text)

index_token = tokenizer.convert_tokens_to_ids(token_text)

print(index_token)

for tup in zip(token_text, index_token):
  print('{:<12} {:>6}'.format(tup[0], tup[1]))

seg_id = [1]*len(token_text)
token_tensor = torch.tensor([index_token])
seg_tensor = torch.tensor([seg_id])
model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)
model.eval()
outputs = 0
with torch.no_grad():
  outputs = model(token_tensor, seg_tensor)

class testModel(nn.Module):
  def __init__(self, bert_model):
    super().__init__()
    self.bert_embedding = bert_model
    self.lstm = nn.LSTM(input_size= 3072, hidden_size=768, num_layers =2)
    self.cnn = nn.Sequential(
        nn.Conv2d(in_channels=2, out_channels=5, kernel_size=5)
    )
    self.cnn1 = nn.Sequential(
        nn.Conv2d(in_channels=2, out_channels=5, kernel_size=2)
    )
  def forward(self,token_tensor, seg_id, h = None, c=None):
    
    embedded = self.embedding_output(token_tensor, seg_id)
    lstm_input = self.process_input(embedded[2])
    print(lstm_input.size())
    if h == None and c==None:
      output, (h,c) = self.lstm(input) 
    else:
      output, (h,c) = self.lstm(input, (h, c))

    print(output.size())
    print(h.size())
    print(c.size())
    cnn_input = output.permute(0,2,1)
    cnn_input.unsqueeze_(dim = 0)
    out = self.cnn(cnn_input)
    print(out.size())
    out2 = self.cnn1(cnn_input)
    print(out2.size())

  def embedding_output(self, token, seg):
    self.bert_embedding.eval()
    with torch.no_grad():
      output = self.bert_embedding(token, seg)
      return output

  def process_input(self, input):
    token_embd = torch.stack(input, dim= 0)
    print(token_embd.size())
    sq_token_embd = torch.squeeze(token_embd, dim = 1)
    print(sq_token_embd.size())
    final_t_e = sq_token_embd.permute(1,0,2)
    print(final_t_e.size())

    token_vec_cat = []
    for item in final_t_e:
      cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim = 0)
      token_vec_cat.append(cat_vec)
    print(len(token_vec_cat))
    print(len(token_vec_cat[0]))
    return torch.stack(token_vec_cat)

model_test = testModel(model)

model_test.eval()
token_tensor = torch.tensor([index_token])
seg_tensor = torch.tensor([seg_id])
model_test(token_tensor, seg_tensor)

print(outputs[0].size())
print(outputs[1].size())
print(outputs[2][0].size())
last_hidden_state = outputs.last_hidden_state
h_state = outputs[2]
print(type(h_state))

print(last_hidden_state.size())

token_embd = torch.stack(h_state, dim= 0)

print(token_embd.size())
sq_token_embd = torch.squeeze(token_embd, dim = 2)

print(sq_token_embd.size())

final_t_e = sq_token_embd.permute(2,0,3,1)
print(final_t_e.size())

test = torch.split(final_t_e, 1, dim = 3)

print(len(test))
print(test[0].size())
l = []
for item in test:
  remove_batch_size = torch.squeeze(item, dim = 3)
  l.append(remove_batch_size)
  print(remove_batch_size.size())

print(len(l))

for item in l:
  print(item.size())

# Not going to use this
token_vec_cat = []
for item in l:
  concat_vec = []
  for token in item:
    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim = 0)
    print(cat_vec.size())
    concat_vec.append(cat_vec)
  token_vec_cat.append(torch.stack(concat_vec))

# Not going to use this
print(f'batch size = {len(token_vec_cat)}')
print(f'{len(token_vec_cat[0])} x {token_vec_cat[0][0].size()}' )

lstm = nn.LSTM(input_size= 3072, hidden_size=768, num_layers =2, bidirectional = True)
#h0 = outputs[0]
#input = np.array(token_vec_cat)
#print(input.shape())
input = torch.stack(token_vec_cat)
print(h0.size())
print(input.size())
out, (hn, cn) = lstm(input)

print(out.size())
print(hn.size())
print(cn.size())

# Now I need to permute it to 0,2,1 so I will have the batch size in the end 
cnn_input = out.permute(0,2,1)
cnn_input.unsqueeze_(2)

print(cnn_input.size())

